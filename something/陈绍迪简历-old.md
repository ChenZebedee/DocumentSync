# 联系方式

- 手机：17826800434
- Email：chenzebedee@gmail.com
- QQ/微信号：331471867

# 个人信息

 - 陈绍迪/男/1995 
 - 本科/杭州电子科技大学 
 - 工作年限：1年
 - Github：https://github.com/ChenZebedee
 - 期望职位：大数据工程师 ETL工程师
 - 期望城市：杭州

# 工作经历

## 软通动力科技有限公司 （ 2021年10月 ~ 至今 ）

### 工作职责

1. 基于nuwa框架对端接口开发及管理台后端开发
2. 把控项目的发布流程，以及解决漏洞等问题
3. 展示广告数仓开发，包含离线数据以及实时数据开发

## 医惠科技有限公司 （ 2019年9月 ~ 2021年10月 ）

### 工作职责

1. 大数据环境搭建，维护，优化
2. 大数据应用开发
3. 数据转换应用开发

## 杭州玛瑙湾科技有限公司 （ 2018年6月 ~ 至今 ）
### 工作职责
1. 大数据环境搭建，维护，优化
2. 离线数据同步
3. 数据 etl 清理
4. shell 脚本编写
5. 实时同步数据处理,并输出到指定表，用于国家数据上报
6. MapReduce 开发

## 北京思特奇 ( 2017年9月 ~ 2018年5月 )
### 工作职责
1. Hive开发，提供局方所需数据。
2. Mapreduce开发，对数据进行处理，并提供给局方
3. Shell脚本编写
4. Python脚本编写
5. Sqoop脚本编写
6. Hbase的MapReduce开发


# 项目经历

### 展示广告数仓建设

###### 项目描述

这不是一个标准的电商数仓建设，一切以结果为导向，主要进行了1. 推荐算法的数据处理、2. 频率频度分析、3. 基础报表等结果的数据预处理以及中间过程的数据处理逻辑，以及基础大宽表的开发，如物料表、日志表、点展表、用户基础特征表、转化表，通过数据的逻辑处理对宽表进行过滤筛选最终产生结果表，可直接用于展示

###### 技术应用： Shell + Spark + HDFS + OBS + Kafka + Hive + HBase + RCMFlow2 + Dcs(Redis)

###### 主要工作职责

1. 项目管理 - 项目的发版，以及代码规范的把控
2. 从Obs或Hdfs获取数据，处理之后导入Hive中
3. Hive获取数据，转成csv提供给第三方使用
4. 从Kafka获取数据实时流处理-计算完之后推送到redis中



##### 医院大数据平台

###### 项目描述

在医院信息化建设中，数据中心建设是最重要的核心部分。但是目前国家还没有出台相关标准、各厂商建设数据中心的目标也不统一，导致医院数据中心建设的效果良莠不齐、为医院带来的价值有限，离医院对数据中心的期望目标还有不少的差距。还存在以下问题：1. 数据中心建设需求不明确、目标感不强。 2. 数据条理性、结构性不强，数据质量无法验证。3. 数据中心缺乏管理工具，可视化程度不强。4. 大数据存储，数据实时性等技术参数较难真正达到
而我们为了建设体系化、层次化、模型化、标准化、集中化、合理化的数据中心。解决上诉问题，为医院进行的大数据应用平台的搭建与开

###### 技术应用： oracle + ogg + Spring Boot + Kafka + HBase + Ambari

###### 主要工作职责

1. 构建ods数据库,cdr数据中心,与Ambari大数据环境
2. 构建源端数据库到达ods的ogg数据同步链路， ods 数据实时同步到 cdr 数据中心与 HBase 的数据链路
3. ods,cdr,Ambari一键脚本开发
4. ogg推送Kafka延迟堆积等问题解决
5. HBase 预分区处
6. 大数据组件应用开发


## 公司数据平台架构建设
### 项目描述
   由于公司的业务发展，数据量日积月累，以及现在的大量增长，传统数据库 MySQL 不能很好的解决千万级的表的复杂查询，并且为了未来的业务发展，准备对整个数据平台进行建设。建设效果如下图：
   
   ![数据平台](https://s2.ax1x.com/2019/08/14/mFm3l9.png)
### 主要工作职责
1. 对于数据平台的规划，并计划每个季度准备落地的平台组件
2. 实现部分平台落地的推动与具体实施
3. 给开发人员普及方便数据利用的数据存储方式
4. 实施如上图中的部分组件

## 风控评分卡模型
### 项目描述
   公司为互联网金融公司，涉及小贷业务，所以风控方需要机申模型，来削减人力成本，由于目标数据需求为每个订单号对应一条数据，而原始数据中每个订单号对应着一条或多条的数据，而多条数据中还要把其中某一列转为列名，与其他列组合形成一条新的数据。最终要输出 800+ 的字段。
   
例：

| id |column1|column2|column3|
|:--:|:-----:|:-----:|:-----:|
|  1 |   a   |  dd   |  ff   |
|  2 |   a   |  cc   |  ee   |
|  3 |   b   |  dd   |  jj   |
 
 变成

 |column_1|dd_column3|cc_column3|
 |:------:|:--------:|:--------:|
 |   a    |    ff    |   ee     |
 |   b    |    jj    |          |

### 技术应用：hadoop + hive + hbase + spark
1. 建模维度宽表字段整理
2. MapReduce/Spark 数据处理程序
3. HBase rowKey 设置
4. HBase 批量获取数据小程序
5. Docker 封装HBase、Spark、Canal容器，各自独立
6. Canal配置按表同步数据到Kafka中，开发Kafka消费者代码，数据存入HBase中
7. 日活数据分析，通过HSQL分析日活数据
8. 联系人碰撞分析，基于HBase开发Spark计算代码
9. 评分卡模型，开发MapReduce程序
10. 数据实时同步上报网信办


## canal 实时同步
### 项目描述
1. 出于安全考虑，所有数据开发均需要进行数据脱敏加密，而原先通过 shell 实现的 T+1 方案对线上 MySQL 压力较大，所以新增 canal 实时同步方案
2. 由于国家网信办要求，提供 P2P 公司的相关业务数据，需实时推送。通过 canal 实现 binlog 的数据同步，再用 spring for apache kafka 将源数据转为目标数据，之后入库
### 技术应用：spring boot +  kafka + canal + mysql
1. canal 源码添加日志
2. kafka 搭建
3. spring boot 开发消费端程序
4. 数据脱敏功能实现
5. 上报数据消费端开发，重点在于格式转换

## 大数据运维
### 项目描述
1. 由于千万级的表进行多表关联时不能用 MySQL 很好的解决，于是就有了大数据环境搭建的工作
2. 为了缓解线上服务器压力，将月报日报等定时任务，将用大数据组件实现
3. 大数据组件的搭建、维护和优化
4. 出于安全与方便部署的考虑，采用 docker 容器技术实现集群搭建
### 技术应用：hadoop + hive + hbase + zookeeper + spark  + sqoop + docker
1. 硬件规划
2. 搭建大数据环境
3. 性能压测
4. 性能优化，通过参数控制，以及 hadoop 自带的资源调度规则
5. hive on tez,hive on spark
6. 运营周报日报等定时任务 shell 编写

##### 业务经理人行为轨迹分析及万号手机版动态日报平台

###### 项目描述

根据 10000 号大屏制作方便各地业务员查看的手机版动态日报平台，以及对业务经理的行为轨迹分析。主要任务如下：
1. 10000 当月话务情况、人工话务走势、各地市话务情况(日均)、人工话务热点。
2. 全省及区域性故障实时预警、宽带故障申告走势、宽带故障原因分类
3. 主量业务发展情况、精准营销转化情况、重点业务发展情况、精准营销转化商机类型
4. 咨询投诉万用户工单率、投诉热点、百万用户越级申诉率
5. 业务经理签退补偿与工作有效性判断

###### 技术应用： Hadoop + Hive + MySQL + HBase + Oracle + OGG + Kafka

###### 主要工作职责

1. 通过MR实现业务经理的签退功能
2. 通过Hive对业务人员进行工作范围确认
3. FTP同步源端数据，导入HIVE中
4. Hive分析数据之后导入MySQL中，用于展示
5. OGG同步Oracle数据至HBase中，通过hive进行分析处理，分析结果存入新的表中
6. 监控HDFS文件变化，使数据同步到Kafka中
7. 通过Canal同步源端Mysql数据到Kafka中
8. 通过SparkStraming进行Kafka的数据消费。

  
# 技能清单
以下均为我熟练使用的技能

- 数据库相关：MySQL
- 版本管理、文档和自动化部署工具：Git
- 编程语言：java/shell/scala
- 大数据相关：hadoop/hive/hbase/spark/kafka/zookeeper
- 容器相关：docker

# 自我介绍
我是一个比较喜欢折腾的人，自己在家喜欢玩玩各种东西，比如说星际矿难时，买了一台二手星际魔盒装黑群晖，用于网盘同步，自己电脑也经常在 windows 和 linux 系统之间重复重装，我也乐于助人，如果同事有任何问题，我都会积极帮助，尤其是技术上的问题，但是我更喜欢的是讨论形式的，大家来个头脑风暴，我的不足可以从中得到进步。

# 致谢
感谢您花时间阅读我的简历，期待能有机会和您共事。
