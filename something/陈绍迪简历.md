#### 联系方式

- 手机/微信号：13175121637
- Email：chenzebedee@gmail.com

#### 个人信息

 - 陈绍迪/男/1995
 - 本科/杭州电子科技大学信息工程学院
 - 工作年限：4年
 - 期望职位：大数据工程师
 - 期望城市：杭州


#### 技能清单

1. 基本的linux命令与shell脚本编写，docker容器化的使用与dockerFile编写与docker compose使用
2. 熟悉大数据的组件与应用：如Hadoop、Spark等计算模型; Hive数仓工具; HBase、redis等存储工具; kafka、canal、OGG数据工具; kettle、datax等数据同步工具
3. 有良好的 JAVA 编码规范，熟练使用spring-boot全家桶进行基本的开发; 熟悉 scala、Python 等语言
4. 熟练使用Git与SVN版本管理工具
5. 熟悉 MySQL，Oracle 等数据库，熟练使用SQL进行开发，有MySQL运维优化经验

#### 工作经历

##### 软通动力科技有限公司 （ 2021年10月 ~ 2022年10月 ）

###### 商品推荐系统

任职期间，我参与了购物云商品推荐的项目开发（这部分是大家都会写的）。作为核心程序员，我不但完成了后端接口、离线数据处理、实时数据处理的开发工作，更提出了高效的组件级缓存系统，通过redis有效缓解了高频查询压力，以及数据处理中的数据热点问题。（这部分是很多同学忘掉的，要写出你在这个项目中具体负责的部分，以及你贡献出来的价值。）在优化过后，API接口性能从100QPS提升到500QPS，服务器由30台减少到8台，离线队列的压力骤减50%（通过量化的数字来增强可信度）。入职半年后我升任购物云项目负责人，带领一个3人小组支持着新的业务数据接入，以及项目发版与质量管理（这就是Benefit。你能带给前雇主的价值，也就是你能带给新雇主的价值。）

##### 医惠科技有限公司 （ 2019年9月 ~ 2021年10月 ）

###### 医疗大数据中心

在职期间，我参与了医院数据的大数据平台的系统建设（这部分是大家都会写的）。作为核心人员，我不但完成了大数据的平台、数据仓库的建设工作，并且通过预计算rowkey、ogg批量压缩推送kafka等优化方案。（这部分是很多同学忘掉的，要写出你在这个项目中具体负责的部分，以及你贡献出来的价值。）在执行优化方案之后，kafka消费堆积发生次数减少了80%。（通过量化的数字来增强可信度）。在职期间是大数据平台的负责人，和同组同事承受着5家医院的大数据平台和数仓的建设（这就是Benefit。你能带给前雇主的价值，也就是你能带给新雇主的价值。）

##### 杭州玛瑙湾科技有限公司 （ 2018年6月 ~ 2019年9月 ）

在职期间，我参与了手机XX网发布系统WAPCMS的开发（这部分是大家都会写的）。作为核心程序员，我不但完成了网站界面、调度队列的开发工作，更提出了高效的组件级缓存系统，通过碎片化缓冲有效的提升了系统的渲染效率。（这部分是很多同学忘掉的，要写出你在这个项目中具体负责的部分，以及你贡献出来的价值。）在该系统上线后，Web前端性能从10QPS提升到200QPS，服务器由10台减少到3台（通过量化的数字来增强可信度）。2008年我升任WAPCMS项目负责人，带领一个3人小组支持着每天超过2亿的PV（这就是Benefit。你能带给前雇主的价值，也就是你能带给新雇主的价值。）




##### 业务经理人行为轨迹分析及万号手机版动态日报平台

###### 项目描述

根据 10000 号大屏制作方便各地业务员查看的手机版动态日报平台，以及对业务经理的行为轨迹分析。主要任务如下：
1. 10000 当月话务情况、人工话务走势、各地市话务情况(日均)、人工话务热点。
2. 全省及区域性故障实时预警、宽带故障申告走势、宽带故障原因分类
3. 主量业务发展情况、精准营销转化情况、重点业务发展情况、精准营销转化商机类型
4. 咨询投诉万用户工单率、投诉热点、百万用户越级申诉率
5. 业务经理签退补偿与工作有效性判断

###### 技术应用： Hadoop + Hive + MySQL + HBase + Oracle + OGG + Kafka

###### 主要工作职责

1. 通过MR实现业务经理的签退功能
2. 通过Hive对业务人员进行工作范围确认
3. FTP同步源端数据，导入HIVE中
4. Hive分析数据之后导入MySQL中，用于展示
5. OGG同步Oracle数据至HBase中，通过hive进行分析处理，分析结果存入新的表中
6. 监控HDFS文件变化，使数据同步到Kafka中
7. 通过Canal同步源端Mysql数据到Kafka中
8. 通过SparkStraming进行Kafka的数据消费。
